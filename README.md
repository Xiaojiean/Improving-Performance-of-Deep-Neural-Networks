# Improving-Deep-Network
We will write codes for different ways to initialize the parameters, optimization checking, regularization techniques like dropout and L2 and other optimization techniques like mini_batch, stochastic gradient descent and Adam. All these codes will be written from scratch without using any in built ML or deep learning libraries or frameworks 
Here we have folder for each intialization techniques, optimization checking, regularization technique and Optimiztion techniques
